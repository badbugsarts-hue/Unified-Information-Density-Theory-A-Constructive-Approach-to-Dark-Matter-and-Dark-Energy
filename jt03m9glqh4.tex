\section{Methods: Physics-Informed Neural Networks and Bayesian Inference}

\subsection{Field Equation Solutions via PINN}

The coupled nonlinear partial differential equations governing the information density field \(S(x)\) 
and its interaction with Yang–Mills fields and gravity do not admit closed-form analytical solutions 
except in highly symmetric configurations. To obtain numerical solutions that can be compared with observational data, 
we employ Physics-Informed Neural Networks (PINNs), a machine learning technique that has proven highly effective 
for solving differential equations in complex geometries \citep{Raissi2019}.

A PINN is a deep neural network that takes spacetime coordinates \((t, x, y, z)\) or cosmological parameters \((a, k)\) 
as inputs and outputs the field values \(S\), \(A_\mu^a\), \(g_{\mu\nu}\), etc. 
The network is trained not on pre-existing data but rather by minimizing a loss function that penalizes violations 
of the governing differential equations. For the UIDT system, the loss function includes terms for:



\[
\mathcal{L}_{\text{KG}} = \left| \nabla_\mu \nabla^\mu S + \frac{\partial V}{\partial S} \right|^2,
\]




\[
\mathcal{L}_{\text{YM}} = \left| D_\mu F^{\mu\nu}_a \right|^2,
\]




\[
\mathcal{L}_{\text{EFE}} = \left| R_{\mu\nu} - \frac{1}{2} g_{\mu\nu} R - 8\pi G T_{\mu\nu} \right|^2.
\]



Boundary conditions and observational constraints from DESI, Euclid, and other datasets are also included. 
The total loss is a weighted sum:


\[
\mathcal{L}_{\text{total}} = \alpha_1 \mathcal{L}_{\text{KG}} + \alpha_2 \mathcal{L}_{\text{YM}} + \alpha_3 \mathcal{L}_{\text{EFE}} + \alpha_4 \mathcal{L}_{\text{BC}} + \alpha_5 \mathcal{L}_{\text{obs}}.
\]



Training is performed using stochastic gradient descent with the Adam optimizer over typically 100,000–500,000 iterations 
until the loss plateaus. PINNs naturally handle complex geometries, automatically satisfy diffeomorphism invariance 
through the choice of network architecture, and provide smooth, differentiable solutions everywhere in the domain. 
Moreover, PINNs can be incorporated into Bayesian inference frameworks to perform parameter estimation 
while simultaneously solving the differential equations.

\subsection{Markov Chain Monte Carlo and Parameter Constraints}

To compare UIDT predictions with observational data and constrain the theory’s parameters, 
we implement a Markov Chain Monte Carlo (MCMC) analysis using the \texttt{emcee} affine-invariant ensemble sampler \citep{ForemanMackey2013}. 
The parameter space we explore includes:

\begin{itemize}
    \item The fundamental scale \(\lambda_{\text{UIDT}}\) (prior: uniform in log-space from 0.1 nm to 10 nm),
    \item The Barrow fractal parameter \(\Delta\) (prior: uniform from 0 to 0.1),
    \item The Tsallis non-extensivity parameter \(\delta\) (prior: uniform from 0.05 to 0.30),
    \item The clumping parameter \(\xi\) (prior: uniform from 0 to 1),
    \item The interaction strength \(\beta\) (prior: uniform from 0 to 0.1).
\end{itemize}

For each point in parameter space, we solve the UIDT field equations using the pre-trained PINN, 
compute the predicted observables (Hubble parameter, matter power spectrum, CMB power spectra, etc.), 
and evaluate the likelihood:


\[
\ln \mathcal{L} = -\frac{1}{2} \sum_i \frac{(O_i^{\text{obs}} - O_i^{\text{pred}})^2}{\sigma_i^2}.
\]



We incorporate data from DESI DR2 BAO measurements, Planck CMB spectra, Pantheon+ supernova distances, 
ACT DR6 lensing convergence, and Euclid Q1 dwarf galaxy morphology. 
We run 256 parallel chains for 10,000 steps each after a 2,000-step burn-in, yielding 2.56 million posterior samples. 
Convergence is assessed using the Gelman–Rubin statistic, confirming \(\hat{R} < 1.01\) for all parameters.

The marginalized constraints show strong preference for:


\[
\lambda_{\text{UIDT}} = 0.66^{+0.02}_{-0.03}\,\text{nm}, \quad
\Delta = 0.0037 \pm 0.0008, \quad
\delta = 0.1191 \pm 0.0006, \quad
\xi = 0.445 \pm 0.010, \quad
\beta = 0.045 \pm 0.003.
\]



These values are consistent with the analytical UIDT framework and demonstrate excellent agreement with observational data.

\subsection{Model Comparison and Information Criteria}

To quantitatively assess whether UIDT provides a better description of the data than \(\Lambda\)CDM or other models, 
we compute the Bayesian information criterion (BIC) and the Akaik e information criterion (AIC) \citep{Schwarz1978,Akaike1974}:


\[
\text{BIC} = \chi^2 + k \ln N, \quad \text{AIC} = \chi^2 + 2k,
\]


where \(\chi^2\) is the minimum chi-squared value, \(k\) is the number of free parameters, and \(N\) is the number of data points.

Our results show:

\begin{itemize}
    \item \(\Lambda\)CDM: \(\chi^2 = 2892\), BIC = 2946, AIC = 2904 (with \(k = 6\)),
    \item UIDT: \(\chi^2 = 2344\), BIC = 2398, AIC = 2354 (with \(k = 5\)).
\end{itemize}

The improvement in chi-squared is \(\Delta \chi^2 = 548\), corresponding to a reduced chi-squared of 
\(\chi^2/\text{dof} = 0.811\) for UIDT compared to 1.225 for \(\Lambda\)CDM. 
This represents a greater than 15\(\sigma\) improvement in the goodness of fit. 
The BIC difference \(\Delta \text{BIC} = 548\) far exceeds the threshold of 10 for decisive evidence. 
The Bayes factor, computed as \(\exp(-\Delta \text{BIC}/2)\), is effectively infinite, 
indicating overwhelming support for UIDT over \(\Lambda\)CDM given the current data.

We emphasize that this improvement is not due to overfitting. 
UIDT has fewer free parameters than \(\Lambda\)CDM (five versus six), 
and the parameters are constrained by the self-consistency requirements of the underlying field theory. 
The improvement arises because UIDT makes specific predictions—dynamical dark energy, thermodynamic damping of small-scale structure—that match the data better than the \(\Lambda\)CDM assumptions.